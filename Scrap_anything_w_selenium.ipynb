{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T15:12:42.956983Z",
     "start_time": "2020-01-20T14:21:06.969221Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "news_contents = []\n",
    "n=1\n",
    "df=()\n",
    "df3 = pd.read_csv('urls_to_scrap.csv')\n",
    "df3 = df3.sample(frac=1).reset_index(drop=True).dropna()\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "m=len(df3)\n",
    "\n",
    "for url in df3['URL']:\n",
    "    print(\"Converting \" + url +\"  URL Number ---> \" + str(n)+\" (Out of ---> \" + str(m)+\")\")\n",
    "    n=n+1\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver',chrome_options=options)\n",
    "    driver.get(url)\n",
    "    soup1 = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "    #coverpage_news = soup1.find_all(\"div\", class_=\"text\")\n",
    "    coverpage_news = soup1.find_all(\"p\")\n",
    "    \n",
    "    list_paragraphs = []\n",
    "    final_article = []\n",
    "    for p in np.arange(0, len(coverpage_news)):\n",
    "        paragraph = coverpage_news[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "    fname = str(n)+'Values.txt'\n",
    "    with io.open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:20:31.895266Z",
     "start_time": "2020-01-20T14:20:31.889285Z"
    }
   },
   "source": [
    "### Scraping to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T10:23:52.761809Z",
     "start_time": "2020-01-14T10:18:52.883429Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "news_contents = []\n",
    "n=1\n",
    "df=()\n",
    "df3 = pd.read_csv('urls_to_scrap.csv')\n",
    "df3 = df3.sample(frac=1).reset_index(drop=True).dropna()\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "m=len(df3)\n",
    "\n",
    "for url in df3['URL']:\n",
    "    print(\"Converting \" + url +\"  URL Number ---> \" + str(n)+\" (Out of ---> \" + str(m)+\")\")\n",
    "    n=n+1\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver',chrome_options=options)\n",
    "    driver.get(url)\n",
    "    soup1 = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "    #coverpage_news = soup1.find_all(\"div\", class_=\"text\")\n",
    "    coverpage_news = soup1.find_all(\"p\")\n",
    "    \n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(coverpage_news)):\n",
    "        paragraph = coverpage_news[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "    news_contents.append(final_article)\n",
    "    df = pd.DataFrame(news_contents)\n",
    "df\n",
    "header=['content']\n",
    "df.to_csv('content_for_analysis1.csv', columns = header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T12:28:15.073776Z",
     "start_time": "2020-01-14T12:28:14.588374Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 80000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T03:13:42.027213Z",
     "start_time": "2019-12-25T03:13:42.021229Z"
    }
   },
   "source": [
    "# scraping trip advisor reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape a destination page to get all reviews pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T16:59:35.925932Z",
     "start_time": "2020-01-05T16:58:02.129978Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "url=\"https://www.tripadvisor.com/Attractions-g187147-Activities-Paris_Ile_de_France.html\"\n",
    "profile = webdriver.FirefoxProfile()\n",
    "# We set the coordinate of where we want to be.\n",
    "profile.set_preference(\"geo.wifi.uri\", 'data:application/json,{\"location\": {\"lat\": 38.912650, \"lng\":-77.036185}, \"accuracy\": 20.0}')\n",
    "# This line is necessary to avoid to prompt for geolocation authorization.\n",
    "profile.set_preference(\"geo.prompt.testing\", True)\n",
    "driver = webdriver.Firefox(profile)\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "list_paragraphs=[]\n",
    "number_of_pages = int(driver.find_elements_by_xpath(\"//a[contains(@class,'pageNum')]\")[5].text)\n",
    "for i in range(number_of_pages - 1):\n",
    "    time.sleep(2)\n",
    "    for ii in driver.find_elements_by_xpath('//a[contains(@href, \"Review\")]'):\n",
    "        paragraph = ii.get_attribute('href')\n",
    "        list_paragraphs.append(paragraph)\n",
    "    clear_output()\n",
    "    print(list_paragraphs)\n",
    "    element = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//a[contains(@class ,\"nav\")]')))\n",
    "    driver.find_element_by_partial_link_text('Next').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T17:15:10.425626Z",
     "start_time": "2020-01-05T17:15:10.407675Z"
    }
   },
   "outputs": [],
   "source": [
    "list_paragraphs=pd.DataFrame(list_paragraphs)\n",
    "list_paragraphs=list_paragraphs.rename(columns={0: \"URL\",})\n",
    "list_paragraphs=pd.DataFrame(list_paragraphs)\n",
    "#list_paragraphs=list_paragraphs.rename(columns={\"content\": \"URL\",})\n",
    "list_paragraphs=list_paragraphs.URL.str.replace(\"#REVIEWS\",\"\")\n",
    "list_paragraphs=pd.DataFrame(list_paragraphs)\n",
    "list_paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T17:17:44.547529Z",
     "start_time": "2020-01-05T17:17:44.523565Z"
    }
   },
   "outputs": [],
   "source": [
    "df=list_paragraphs\n",
    "df=df[~df['URL'].str.contains('VacationRentals')]\n",
    "df=df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T12:08:11.978706Z",
     "start_time": "2019-12-29T12:08:11.967763Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('urls_to_scrap2.csv')\n",
    "\n",
    "df=pd.concat([list_paragraphs,df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T22:30:40.662936Z",
     "start_time": "2020-01-05T20:45:25.858927Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "profile = webdriver.FirefoxProfile()\n",
    "# We set the coordinate of where we want to be.\n",
    "profile.set_preference(\"geo.wifi.uri\", 'data:application/json,{\"location\": {\"lat\": 38.912650, \"lng\":-77.036185}, \"accuracy\": 20.0}')\n",
    "# This line is necessary to avoid to prompt for geolocation authorization.\n",
    "profile.set_preference(\"geo.prompt.testing\", True)\n",
    "driver = webdriver.Firefox(profile)\n",
    "\n",
    "\n",
    "# function to check if the button is on the page, to avoid miss-click problem\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(@class, 'list-parts-ExpandableReview')]\"))).click()\n",
    "#driver.find_element_by_xpath(\"//span[contains(@class, 'list-parts-ExpandableReview')]\").click()\n",
    "df3 = df\n",
    "df_Trip_final=pd.DataFrame()\n",
    "a=0\n",
    "b=0\n",
    "news_contents_Trip = []\n",
    "for url in df['URL']:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    # change the value inside the range to save more or less reviews\n",
    "    try:\n",
    "        number_of_pages = int(driver.find_elements_by_xpath(\"//a[contains(@class,'pageNum')]\")[5].text)\n",
    "        number_of_pages = int(number_of_pages/10)\n",
    "    except:\n",
    "        number_of_pages=2\n",
    "        continue\n",
    "    for i in range(number_of_pages - 1):\n",
    "        time.sleep(5)\n",
    "        count=a+b\n",
    "        print(\"done\"+\"  \"+str(i)+\" \"+\"pages out of:  \"+ str(number_of_pages))\n",
    "        print(\"============================\")\n",
    "        print(\"===========================\")\n",
    "        print('error count:   '+str(count))\n",
    "        list_paragraphs = []\n",
    "        if (check_exists_by_xpath(\"//div[contains(@class, 'list-parts-ExpandableReview')]\")):\n",
    "            # to expand the review\n",
    "            try:\n",
    "                element = WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(@class ,\"_36B4Vw6t\")]')))\n",
    "                element.click();\n",
    "                time.sleep(0.5)\n",
    "                ids=driver.find_elements_by_xpath('//q[contains(@class ,\"review-list-parts\")]')\n",
    "            except:\n",
    "                print(\"error 1\")\n",
    "                a=a+1\n",
    "                pass\n",
    "            for ii in range(len(ids)):\n",
    "                try:\n",
    "                    element = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//q[contains(@class ,\"review-list-parts\")]')))\n",
    "                    time.sleep(0.5)\n",
    "                    paragraph = ids[ii].text\n",
    "                    print(\"done\"+\"  \"+str(ii)+\" \"+\"reviews\")\n",
    "                    list_paragraphs.append(paragraph)\n",
    "                    news_contents_Trip=pd.DataFrame(list_paragraphs)\n",
    "                    #final_article_Trip = \" \".join(list_paragraphs)\n",
    "                except:\n",
    "                    print(\"error 2\")\n",
    "                    b=b+1\n",
    "                    pass\n",
    "            # to change the page\n",
    "            time.sleep(0.5)\n",
    "            clear_output()\n",
    "            driver.find_element_by_partial_link_text('Next').click()\n",
    "        df_Trip_final = pd.concat([df_Trip_final,news_contents_Trip])\n",
    "        clear_output()\n",
    "        print(df_Trip_final.tail())\n",
    "        \n",
    "df_Trip_final.to_csv('content_for_analysis_trip.csv')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T22:57:01.684350Z",
     "start_time": "2020-01-05T22:57:01.661437Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Trip_final.to_csv('content_for_analysis.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
